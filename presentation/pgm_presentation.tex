\input{preamble_theme}

% Presentation data
% ====================================================

\title[PGM Project Presentation]{* * * PGM: Final Project Presentation * * * POS-Tagging and NER}
\institute{TU Darmstadt}
\author{C. Biehl, F. Otto, D. Wehner}
\date{\today}
\prefix{PGM}

% BEGIN OF DOCUMENT
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage

% Agenda
%______________________________________________________________________
\begin{frame}{Agenda}
	\tableofcontents
\end{frame}


\section{Task Description}

% The Task
\begin{frame}{Task Description}{}
	\divideTwo{0.59}{
		\begin{itemize}
			\item The task was to use probabilistic graphical models for:
			\begin{itemize}
				\item POS-Tagging (Part-of-Speech Tagging)
				\item Named Entity Recognition (NER)
			\end{itemize}
			\item We used the following models:
			\begin{itemize}
				\item Na\"{i}ve Bayes (baseline model)
				\item HMMs (Hidden Markov models)
				\item CRFs (Conditional Random Fields)
			\end{itemize}
		\end{itemize}
	}{0.39}{
		\begin{figure}
			\fbox{\includegraphics[scale=0.05]{img/bayes_theorem}}
		\end{figure}
	}
\end{frame}


% Part of Speech Tagging
\begin{frame}{Part of Speech Tagging}{}
	\begin{figure}
		\includegraphics[scale=0.4]{img/pos_tagging}
		\caption{Part of speech tagging}
	\end{figure}
\end{frame}


% Named Entity Recognition
\begin{frame}{Named Entity Recognition}{}
	\begin{figure}
		\includegraphics[scale=0.35]{img/ner}
		\caption{Named entity recognition}
	\end{figure}
\end{frame}

\section{Process and Tools}

% Process and Tools
\begin{frame}{Process and Tools}{}
	\begin{figure}
		\includegraphics[scale=0.4]{img/nlp_pipeline}
		\caption{NLP-Pipeline}
	\end{figure}

	\begin{itemize}
		\item POS tagging is one of the earliest stages in the NLP pipeline and serves as input for most
			downstream tasks
		\item E.\,g. NER: POS tags can be used as a feature 
	\end{itemize}
\end{frame}


% Data Set
\begin{frame}{Data Set}{}
	\divideTwo{0.59}{
		\begin{itemize}
			\item We used the \textbf{Groningen Meaning Bank}
			\item \url{http://gmb.let.rug.nl/data.php}
			\item Corpus of public domain texts:
			\begin{itemize}
				\item 1.4 milion words
				\item automatically produced, manually corrected (silver labels)
			\end{itemize}
		\end{itemize}
	}{0.39}{
		\begin{figure}
			\fbox{\includegraphics[scale=0.5]{img/groningen_meaning_bank}}
		\end{figure}
	}
\end{frame}


% Tag Distribution
\begin{frame}{Tag Distribution}{}
	\divideTwo{0.49}{
		\begin{figure}
			\includegraphics[scale=0.2]{img/pos_tag_dist}
			\caption{POS tag distribution}
		\end{figure}
	}{0.49}{
		\begin{figure}
			\includegraphics[scale=0.195]{img/entity_tag_dist}
			\caption{NER tag distribution}
		\end{figure}
	}
\end{frame}


\section{Results and Comparison}

% Results
\begin{frame}{Results}{POS Tagging}
	\divideTwo{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{Na\"{i}ve Bayes} [POS]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc (w / s)}
				\\ \hline\hline
				w, l, s 				& 	0.927				& 	0.927 / 0.240
				\\ \hline
				w, l, s, uwf				&	0.928				&	0.928 / 0.241
				\\ \hline
				w, l, s, bf 				& 	0.957 				&	0.957 / 0.467
				\\ \hline
				w, l, s, uwf, bf			& 	0.949 				& 	0.949 / 0.369
				\\ \hline
			\end{tabular}}
		\end{table}
	}{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{HMM} [POS]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc (w / s)}
				\\ \hline\hline
				word sequence 		&	0.837 				& 	0.837 / 0.496
				\\ \hline
			\end{tabular}}
		\end{table}
	}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features
	
	\vspace*{2mm}
	\normalsize
	\textit{Bi-LSTM Baseline:} 0.950 [Bjerva et al., 2016]
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{POS Tagging}
	\begin{table}[h]
		\scalebox{1.0}{
		\begin{tabular}{| c | c | c |}
			\hline
			\multicolumn{3}{| c |}{\textbf{CRF} [POS]}
			\\ \hline\hline
			\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc (w / s)}
			\\ \hline\hline
			w, l, s 				& 	0.974				&  	0.974 / 0.607	
			\\ \hline
			w, l, s, uwf				&	0.980				&	0.980 / 0.749
			\\ \hline
			w, l, s, bf 				& 	0.978		 		&	0.978 / 0.669
			\\ \hline
			w, l, s, uwf, bf			& 	0.985 				& 	0.985 / 0.740
			\\ \hline
		\end{tabular}}
	\end{table}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features
\end{frame}


% POS: Confusion Matrix CRF (all features)
\begin{frame}{POS: Confusion Matrix CRF (all features)}{}
	\begin{figure}
		\includegraphics[scale=0.08]{img/cm_pos}
		\caption{Named entity recognition}
	\end{figure}
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{Named Entity Recognition}
	\divideTwo{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c | }
				\hline
				\multicolumn{3}{| c |}{\textbf{Na\"{i}ve Bayes} [NER]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc (w / s)}
				\\ \hline\hline
				w, l, s				& 	0.921				& 	0.921 / 0.262
				\\ \hline
				w, l, s, uwf				&	0.922				&	0.922 / 0.237
				\\ \hline
				w, l, s, uwf, pos			& 	0.921 				&	0.921 / 0.240
				\\ \hline
				w, l, s, uwf, pos, bf		& 	0.929 				& 	0.929 / 0.288
				\\ \hline
			\end{tabular}}
		\end{table}
	}{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{HMM} [NER]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc (w / s)}
				\\ \hline\hline
				word sequence			&	0.946				& 	0.946 / 0.523
				\\ \hline
			\end{tabular}}
		\end{table}
	}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
	
	\vspace*{2mm}
	\normalsize
	\textit{LSTM Baseline:} 0.931 [Akbik et al., 2018],  \textcolor{red}{different data set: \textbf{CoNLL03}}
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{Named Entity Recognition}
	\begin{table}[h]
		\scalebox{0.8}{
		\begin{tabular}{| c |  c | c |}
			\hline
			\multicolumn{3}{| c |}{\textbf{CRF} [NER]}
			\\ \hline\hline
			\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy (w / s)}
			\\ \hline\hline
			w, l, s	 			& 	0.965				&	0.965 / 0.604
			\\ \hline
			w, l, s, uwf 			& 	0.969				&  	0.969 / 0.639	
			\\ \hline
			w, l, s, uwf, pos			& 	0.969				&	0.969 / 0.645
			\\ \hline
			w, l, s, uwf, pos, bf		& 	0.974				&	0.974 / 0.693
			\\ \hline
		\end{tabular}}
	\end{table}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
\end{frame}


% NER: Confusion Matrix CRF (all features)
\begin{frame}{NER: Confusion Matrix CRF (all features)}{}
	\begin{figure}
		\includegraphics[scale=0.1]{img/cm_ner}
		\caption{Named entity recognition}
	\end{figure}
\end{frame}


% NER: Random Search for CRF
\begin{frame}{NER: Random Search for CRF}{}
	\divideTwo{0.49}{
		\vspace*{2mm}
		\begin{figure}
			\includegraphics[scale=0.22]{img/crf_hyperparam_search}
			\caption{Random Search CRF}
		\end{figure}
	}{0.49}{
		\begin{itemize}
			\item \texttt{C1}: $L_1$-regularization
			\item \texttt{C2}: $L_2$-regularization
			\item Best parameters found:
			\begin{itemize}
				\item c1: 0.40431785916906465
				\item c2: 0.16525290148249140
			\end{itemize}
		\end{itemize}
	}
\end{frame}


% Comparison
\begin{frame}{Result Comparison}{}
	\begin{itemize}
		\item Naive Bayes confuses \texttt{per-ord} with \texttt{eve-ord}, but HMM does not \\
			$\Rightarrow$ context knowledge!
		\item Naive Bayes predicts \texttt{per-tit} very often
		\item Naive Bayes confuses \texttt{tim-clo} and \texttt{tim-dat} often
		\item HMM incorrectly predicts \texttt{DET} tag (POS) and \texttt{O} tag (NER) very often
		\item \textbf{POS tagging}
		\begin{itemize}
			\item Likely transitions: \texttt{MD} $\Rightarrow$ \texttt{VB}; \texttt{NNS} $\Rightarrow$ \texttt{VBP}
			\item Unlikely transitions: \texttt{DT} $\Rightarrow$ \texttt{DT}; \texttt{VBD} $\Rightarrow$ \texttt{VBD}
		\end{itemize}
		\item \textbf{NER}
		\begin{itemize}
			\item Likely transitions: \texttt{per-giv} $\Rightarrow$ \texttt{per-fam}
			\item Unlikely transitions: \texttt{per-giv} $\Rightarrow$ \texttt{tim-dat}
		\end{itemize}
	\end{itemize}
\end{frame}


% What we have learned...
\begin{frame}{What we have learned...}{}
	\begin{itemize}
		\item Feature engineering is 'dirty work' but \textbf{crucial} for later performance of the model
		\item POS tagging is \textbf{easier} than NER
		\item Using less training data \textbf{hurts performance}, especially on sentence level \\
			(e.\,g. POS: foreign words misclassified, mistakes interjection with adjective)
		\item CRF mostly works \textbf{'out-of-the-box'}; we conducted a random search and noticed only tiny differences 
			($\le 0.03$)
		\item We tried word embeddings with limited success
	\end{itemize}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}