\input{preamble_theme}

% Presentation data
% ====================================================

\title[PGM Project Presentation]{* * * PGM: Final Project Presentation * * * PoS-Tagging and NER}
\institute{TU Darmstadt}
\author{C. Biehl, F. Otto, D. Wehner}
\date{\today}
\prefix{PGM}

% BEGIN OF DOCUMENT
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage

% Agenda
%______________________________________________________________________
\begin{frame}{Agenda}
	\tableofcontents
\end{frame}


\section{Task Description}

% The Task
\begin{frame}{Task Description}{}
	\divideTwo{0.59}{
		\begin{itemize}
			\item The task was to use probabilistic graphical models for:
			\begin{itemize}
				\item POS-Tagging (Part-of-Speech Tagging)
				\item Named Entity Recognition (NER)
			\end{itemize}
			\item We used the following models:
			\begin{itemize}
				\item Na\"{i}ve Bayes (baseline model)
				\item HMMs (Hidden Markov models)
				\item CRFs (Conditional Random Fields)
			\end{itemize}
		\end{itemize}
	}{0.39}{
		\begin{figure}
			\fbox{\includegraphics[scale=0.05]{img/bayes_theorem}}
		\end{figure}
	}
\end{frame}


\section{Process and Tools}

% Process and Tools
\begin{frame}{Process and Tools}{}
	\begin{figure}
		\includegraphics[scale=0.4]{img/nlp_pipeline}
		\caption{NLP-Pipeline}
	\end{figure}

	\begin{itemize}
		\item POS tagging is one of the earliest stages in the NLP pipeline and serves as input for most
			downstream tasks
		\item E.\,g. NER: POS tags can be used as a feature 
	\end{itemize}
\end{frame}


\section{Results and Comparison}

% Results
\begin{frame}{Results}{POS Tagging}
	\divideTwo{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{Na\"{i}ve Bayes} [POS]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy}
				\\ \hline\hline
				w, l, s 				& 	0.927				& 	0.927 / 0.240
				\\ \hline
				w, l, s, uwf				&	0.928				&	0.928 / 0.241
				\\ \hline
				w, l, s, bf 				& 	0.957 				&	0.957 / 0.467
				\\ \hline
				w, l, s, uwf, bf			& 	0.949 				& 	0.949 / 0.369
				\\ \hline
			\end{tabular}}
		\end{table}
	}{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{HMM} [POS]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy}
				\\ \hline\hline
			\end{tabular}}
		\end{table}
	}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{POS Tagging}
	\begin{table}[h]
		\scalebox{1.0}{
		\begin{tabular}{| c | c | c |}
			\hline
			\multicolumn{3}{| c |}{\textbf{CRF} [POS]}
			\\ \hline\hline
			\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy}
			\\ \hline\hline
			w, l, s 				& 	0.973				& 	0.973 / 0.605
			\\ \hline
			w, l, s, uwf				&	0.980				&	0.980 / 0.680
			\\ \hline
			w, l, s, bf 				& 	0.980 				&	0.980 / 0.677
			\\ \hline
			w, l, s, uwf, bf			& 	0.984 				& 	0.984 / 0.734
			\\ \hline
		\end{tabular}}
	\end{table}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{Named Entity Recognition}
	\divideTwo{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c | }
				\hline
				\multicolumn{3}{| c |}{\textbf{Na\"{i}ve Bayes} [NER]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy}
				\\ \hline\hline
				w, l					& 	0.921				& 	0.921 / 0.262
				\\ \hline
				w, l, uwf				&	0.922				&	0.922 / 0.237
				\\ \hline
				w, l, uwf, pos			& 	0.921 				&	0.921 / 0.240
				\\ \hline
				w, l, uwf, pos, bf		& 	0.929 				& 	0.929 / 0.288
				\\ \hline
			\end{tabular}}
		\end{table}
	}{0.49}{
		\begin{table}[h]
			\scalebox{0.8}{
			\begin{tabular}{| c | c | c |}
				\hline
				\multicolumn{3}{| c |}{\textbf{HMM} [NER]}
				\\ \hline\hline
				\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Accuracy}
				\\ \hline\hline
			\end{tabular}}
		\end{table}
	}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
\end{frame}


% Results (Ctd.)
\begin{frame}{Results (Ctd.)}{Named Entity Recognition}
	\begin{table}[h]
		\scalebox{0.8}{
		\begin{tabular}{| c | c | c | c |}
			\hline
			\multicolumn{4}{| c |}{\textbf{CRF} [NER]}
			\\ \hline\hline
			\textbf{Features}		&	\textbf{F1 score}		& 	\textbf{Acc word}	& 	\textbf{Acc sent}
			\\ \hline\hline
		\end{tabular}}
	\end{table}
	\tiny \textbf{Legend:} \\
	\textbf{w}: word, \textbf{l}: lowercase word, \textbf{s}: stem, \textbf{uwf}: unknown word features,
	\textbf{bf}: bigram features, \textbf{pos}: part of speech tags
\end{frame}


% Comparison
\begin{frame}{Result Comparison}{}
	\begin{itemize}
		\item Naive Bayes confuses \texttt{per-ord} with \texttt{eve-ord}, but HMM does not \\
			$\Rightarrow$ context knowledge!
		\item Naive Bayes predicts \texttt{per-tit} very often
		\item Naive Bayes confuses \texttt{tim-clo} and \texttt{tim-dat} often
		\item HMM incorrectly predicts \texttt{DET} tag (POS) very often
		\item HMM incorrectly predicts \texttt{O} tag (NER) very often
	\end{itemize}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}